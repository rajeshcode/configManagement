<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>
  
<!-- The directories for NN, DN and SNN configs -->
  
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>/hadoop/nn1/1,/hadoop/nn1/2</value>
    <final>true</final>
  </property>
  
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>/hadoop/1/data,/hadoop/2/data,/hadoop/3/data,/hadoop/4/data,/hadoop/5/data,/hadoop/6/data,/hadoop/7/data,/hadoop/8/data,/hadoop/9/data,/hadoop/10/data,/hadoop/11/data,/hadoop/12/data</value> 
  </property>
  
  <property>
    <name>dfs.namenode.checkpoint.dir</name>
    <value>/hadoop/nn2/1</value>
    <final>true</final>
  </property>

<!-- The http address -->

<property>
    <name>dfs.namenode.http-address</name>
    <value>nox-srw-nn.vip.ebay.com:50070</value>
    <description>
        The address and the base port where the dfs namenode web ui will listen on.
        If the port is 0 then the server will start on a free port.
    </description>
</property>

<property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>nox-srw-sn.vip.ebay.com:50090</value>
    <description>
        The secondary namenode http server address and port.
        If the port is 0 then the server will start on a free port.
    </description>
</property>

<!-- The Nodes include and exclude -->

 <property>
   <name>dfs.hosts</name>
   <!-- The files containing hosts allowed to connect to namenode -->
   <value>/apache/hadoop/etc/hadoop/hosts</value>
 </property>

 <property>
   <name>dfs.hosts.exclude</name>
   <!-- The files containing hosts allowed to connect to namenode -->
   <value>/apache/hadoop/etc/hadoop/hdfs-exclude</value>
 </property>


  <property>
    <name>dfs.datanode.failed.volumes.tolerated</name>
    <value>3</value>
  </property>

  <property>
    <name>dfs.datanode.balance.bandwidthPerSec</name>
    <value>10485760</value>
  </property>

  <property>
   <!-- Amount of space which HDFS will refuse to use in bytes -->
   <name>dfs.datanode.du.reserved</name>
   <value>107374182400</value> <!-- 100 GB-->
  </property>

<!-- RMERCHIA AISOPS159160 2012-09-25 -->

  <property>
  <name>dfs.heartbeat.interval</name>
  <value>6</value>
  <description>how frequently dn send a heartbeat.</description>
  </property>

<!-- RMERCHIA AISOPS159160 2012-09-25  change to 6 hours on 2012-10-02 -->

  <property>
  <name>dfs.blockreport.intervalMsec</name>
  <value>21600000</value>
  <description>how frequently dn send a blockreport.</description>
  </property>

  <property>
   <name>dfs.namenode.safemode.threshold-pct</name>
   <value>1.0f</value>
   <!-- Allows 10 blocks unreported out of 10,000,000 -->
   <description>
        Specifies the percentage of blocks that should satisfy
        the minimal replication requirement defined by dfs.replication.min.
        Values less than or equal to 0 mean not to start in safe mode.
        Values greater than 1 will make safe mode permanent.
   </description>
</property>

<property>
   <name>dfs.namenode.safemode.extension</name>
   <value>120000</value>
   <!-- 2 minutes -->
   <description> Determines extension of safe mode in milliseconds after the threshold level is reached. </description>
</property>

<property>
<name>dfs.permissions.enabled</name>
<value>true</value>
<description>
If "true", enable permission checking in HDFS.
If "false", permission checking is turned off,
but all other behavior is unchanged.
Switching from one parameter value to the other does not change the mode,
owner or group of files or directories.
</description>
</property>

  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>

<property>
    <name>dfs.blocksize</name>
    <!-- 128mb (default 64m or 67108864) -->
    <value>134217728</value>
</property>

<property>
    <name>dfs.namenode.handler.count</name>
    <value>128</value>
  </property>

  <property>
    <name>dfs.datanode.handler.count</name>
    <value>50</value>
  </property>

<property>
    <name>dfs.datanode.max.transfer.threads</name>
    <value>4096</value>
  </property>

<property>
<name>dfs.namenode.replication.max-streams</name>
  <value>40</value>
</property>

<property>
<name>dfs.webhdfs.enabled</name>
<value>true</value>
</property>

<property>
    <name>dfs.block.local-path-access.user</name>
    <value>hadoop</value>
    <description>the user who is allowed to perform short circuit reads.</description>
</property>

<property>
<name>dfs.block.access.token.enable</name>
<value>true</value>
</property>

<property>
    <name>dfs.namenode.name.dir.restore</name>
    <value>true</value>
</property>

<property>
    <name>dfs.ls.limit</name>
    <value>4096</value>
</property>

<!-- NameNode security config -->
<property>
  <name>dfs.web.authentication.kerberos.keytab</name>
  <value>/etc/hadoop/hadoop.keytab</value>
</property>
<property>
  <name>dfs.namenode.kerberos.internal.spnego.principal</name>
  <value>HTTP/_HOST@APDTEST.EBAY.COM</value>
</property>
<property>
  <name>dfs.namenode.keytab.file</name>
  <value>/etc/hadoop/hadoop.keytab</value>
</property>
<property>
  <name>dfs.namenode.kerberos.principal</name>
  <value>hadoop/_HOST@APDTEST.EBAY.COM</value>
<!-- _HOST will be replaced by the the domain name present in fs.default.name. It is better to use the actual host name  -->
</property>
<property>
  <name>dfs.web.authentication.kerberos.principal</name>
  <value>HTTP/_HOST@APDTEST.EBAY.COM</value>
</property>

<!-- Secondary NameNode security config -->
<property>
  <name>dfs.secondary.namenode.kerberos.internal.spnego.principal</name>
  <value>HTTP/_HOST@APDTEST.EBAY.COM</value>
</property>
<property>
  <name>dfs.secondary.namenode.keytab.file</name>
  <value>/etc/hadoop/hadoop.keytab</value>
</property>
<property>
  <name>dfs.secondary.namenode.kerberos.principal</name>
  <value>hadoop/_HOST@APDTEST.EBAY.COM</value>
</property>
<!-- DataNode security config -->
<property>
  <name>dfs.datanode.data.dir.perm</name>
  <value>700</value>
  </property>
<property>
  <name>dfs.datanode.address</name>
  <value>0.0.0.0:1004</value>
</property>
<property>
  <name>dfs.datanode.http.address</name>
  <value>0.0.0.0:1006</value>
</property>
<property>
  <name>dfs.datanode.keytab.file</name>
  <value>/etc/hadoop/hadoop.keytab</value>
</property>
<property>
  <name>dfs.datanode.kerberos.principal</name>
  <value>hadoop/_HOST@APDTEST.EBAY.COM</value>
  <!-- _HOST will be replaced by the frst domain name mapped to the ip -->
</property>

<property>
  <name>dfs.cluster.administrators</name>
  <value> hdmi-hadoopeng</value>
</property>

<!-- HTTPS SUPPORT -->

<property>
    <name>dfs.https.need.client.auth</name>
    <value>false</value>
    <description>Whether SSL client certificate authentication is required
    </description>
</property>

<property>
    <name>dfs.https.server.keystore.resource</name>
    <value>ssl-server.xml</value>
    <description>Resource file from which ssl server keystore
        information will be extracted
    </description>
</property>

<property>
    <name>dfs.https.client.keystore.resource</name>
    <value>ssl-client.xml</value>
    <description>Resource file from which ssl client keystore
        information will be extracted
    </description>
</property>

<property>
    <name>dfs.namenode.kerberos.https.principal</name>
    <value>hadoop/nox-srw-nn.vip.ebay.com@APDTEST.EBAY.COM</value>
</property>

<property>
    <name>dfs.secondary.namenode.kerberos.https.principal</name>
    <value>hadoop/nox-srw-sn.vip.ebay.com@APDTEST.EBAY.COM</value>
</property>

<property>
    <name>dfs.https.address</name>
    <value>nox-srw-nn.vip.ebay.com:50070</value>
    <description>
        The address and the base port where the dfs namenode web ui will listen on.
        If the port is 0 then the server will start on a free port.
    </description>
</property>

<property>
    <name>dfs.secondary.https.address</name>
    <value>nox-srw-sn.vip.ebay.com:50090</value>
</property>

<property>
    <name>dfs.datanode.https.address</name>
    <value>0.0.0.0:1006</value>
</property>



<property>
    <name>dfs.domain.socket.path</name>
    <value>/var/run/hadoop-hdfs/dn</value>
</property>

<property>
  <name>dfs.client.read.shortcircuit</name>
  <value>true</value>
</property>

<property>
  <name>dfs.namenode.servicerpc-address</name>
  <value>hdfs://nox-srw-nn.vip.ebay.com:8030</value>
</property>

<property>
  <name>dfs.namenode.service.handler.count</name>
  <value>55</value>
</property> 


</configuration>
