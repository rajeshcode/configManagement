<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration xmlns:xi="http://www.w3.org/2001/XInclude">

<!-- i/o properties -->

  <property>
    <name>io.file.buffer.size</name>
    <value>131072</value>
    <description>The size of buffer for use in sequence files.
  The size of this buffer should probably be a multiple of hardware
  page size (4096 on Intel x86), and it determines how much data is
  buffered during read and write operations.</description>
  </property>

<property>
  <description>If users connect through a SOCKS proxy, we don't
   want their SocketFactory settings interfering with the socket
   factory associated with the actual daemons.</description>
   <name>hadoop.rpc.socket.factory.class.default</name>
   <value>org.apache.hadoop.net.StandardSocketFactory</value>
</property>

<property>
  <name>hadoop.tmp.dir</name>
  <value>/tmp/hadoop/hadoop-${user.name}</value>
  <description>A base for other temporary directories.</description>
</property>

<property>
  <name>hadoop.rpc.socket.factory.class.ClientProtocol</name>
  <value></value>
</property>

<property>
  <name>hadoop.rpc.socket.factory.class.JobSubmissionProtocol</name>
  <value></value>
</property>
              
  <property>
    <name>io.serializations</name>
    <value>org.apache.hadoop.io.serializer.WritableSerialization</value>
  </property>

  <property>
    <name>io.compression.codecs</name>
    <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec</value>
  </property>

  <!-- LZO: see http://www.facebook.com/notes/cloudera/hadoop-at-twitter-part-1-splittable-lzo-compression/178581952002 -->
  <property>
      <name>io.compression.codec.lzo.class</name>
      <value>com.hadoop.compression.lzo.LzoCodec</value>
  </property>


<!-- file system properties -->

  <property>
    <name>fs.default.name</name>
    <!-- cluster variant -->
    <value>hdfs://apollo-phx-nn.vip.ebay.com:8020</value>
    <description>The name of the default file system.  Either the
  literal string "local" or a host:port for NDFS.</description>
    <final>true</final>
  </property>

  <property>
    <description>Topology script</description>
    <name>net.topology.script.file.name</name>
    <value>/apache/hadoop/etc/hadoop/topology</value>
    <final>true</final>
  </property>

  <property>
    <name>fs.trash.interval</name>
    <value>10080</value>
    <description>Number of minutes between trash checkpoints.
                 If zero, the trash feature is disabled.
    </description>
  </property>	

  <!-- mobius-proxyagent impersonation configurations -->
<property>
  <name>hadoop.proxyuser.mobius-proxyagent.groups</name>
  <value>hdmi-mm,hdmi-set,hdmi-research,hdmi-technology,hdmi-hadoopeng,hdmi-cs,hdmi-milo,hdmi-appdev,hdmi-siteanalytics,hdmi-prod,hdmi-others,hdmi-sdc,hdmi-finance,hdmi-est,hdmi-cci,hdmi-mptna,hdmi-xcom,hdmi-stu,hdmi-mobile</value>
  <description>Allow user mobius-proxyagent to impersonate any members of the groups </description>
</property>

<property>
    <name>hadoop.proxyuser.mobius-proxyagent.hosts</name>
    <value>10.114.118.13,10.115.201.53</value>
    <description>The mobius-proxyagent can connect from hosts to impersonate a user</description>
</property>

<property>
      <name>hadoop.proxyuser.bridge_adm.groups</name>
      <value>hdmi-mm,hdmi-set,hdmi-research,hdmi-technology,hdmi-hadoopeng,hdmi-cs,hdmi-milo,hdmi-appdev,hdmi-siteanalytics,hdmi-prod,hdmi-others,hdmi-sdc,hdmi-finance,hdmi-est,hdmi-cci,hdmi-mptna,hdmi-xcom,hdmi-stu,hdmi-mobile</value>
      <description>Allow user bridge_adm (Teradata-Hadoop bridge) to impersonate any members of the groups </description>
</property>

<property>
    <name>hadoop.proxyuser.bridge_adm.hosts</name>
    <value>10.103.47.11,10.103.47.12,10.103.47.13,10.103.47.14,10.103.47.15,10.103.47.16,10.103.47.17,10.103.47.18,10.103.47.19,10.103.47.20,10.103.47.21,10.103.47.22,10.103.48.11,10.103.48.12,10.103.48.13,10.103.48.14,10.103.48.15,10.103.48.16,10.103.48.17,10.103.48.18,10.103.48.19,10.103.48.20,10.103.48.21,10.103.48.22,10.103.88.11,10.103.88.12,10.103.88.13,10.103.88.14,10.103.88.15,10.103.88.16,10.103.88.17,10.103.88.18,10.103.88.19,10.103.88.20,10.103.88.21,10.103.88.22,10.103.88.23,10.103.88.24,10.103.88.25,10.103.88.26,10.103.88.27,10.103.88.28,10.103.88.29,10.103.88.30,10.103.88.31,10.103.88.32,10.103.88.33,10.103.88.34,10.103.89.11,10.103.89.12,10.103.89.13,10.103.89.14,10.103.89.15,10.103.89.16,10.103.89.17,10.103.89.18,10.103.89.19,10.103.89.20,10.103.89.21,10.103.89.22,10.103.89.23,10.103.89.24,10.103.89.25,10.103.89.26,10.103.89.27,10.103.89.28,10.103.89.29,10.103.89.30,10.103.89.31,10.103.89.32,10.103.89.33,10.103.89.34</value>
    <description>The bridge_adm user (Teradata-Hadoop bridge) can connect from hosts to impersonate a user</description>
</property>

<property>
    <name>hadoop.proxyuser.hadoop.hosts</name>
    <value>*</value>
</property>

<property>
    <name>hadoop.proxyuser.hadoop.groups</name>
    <value>*</value>
</property>

<property>
   <name>hadoop.proxyuser.sg_adm.groups</name>
   <value>hdmi-etl</value>
   <description>Allow user sg_adm (HDMIT-4462) to impersonate any  members of the groups </description>
</property>

<property>
   <name>hadoop.proxyuser.sg_adm.hosts</name>
   <value>*</value>
   <description>The sg_adm user (HDMIT-4462) can connect from hosts to impersonate a user</description>
</property>

<property>
  <name>hadoop.proxyuser.hive.groups</name>
  <value></value>
  <description>
     Proxy group for Hadoop.
  </description>
</property>

<property>
  <name>hadoop.proxyuser.hive.hosts</name>
  <value></value>
  <description>
     Proxy host for Hadoop.
  </description>
</property>

<property>
  <name>hadoop.proxyuser.oozie.groups</name>
  <value></value>
  <description>
     Proxy group for Hadoop.
  </description>
</property>

<property>
  <name>hadoop.proxyuser.oozie.hosts</name>
  <value></value>
  <description>
     Proxy host for Hadoop.
  </description>
</property>

<property>
        <name>hadoop.proxyuser.hue.hosts</name>
        <value>*</value>
</property>

<property>
        <name>hadoop.proxyuser.hue.groups</name>
        <value>*</value>
</property>

  <property>
    <name>fs.inmemory.size.mb</name>
    <value>256</value>
  </property>


  <property>
    <name>fs.checkpoint.period</name>
    <value>21600</value>
    <description>The number of seconds between two periodic checkpoints.
  </description>
  </property>

  <property>
    <name>fs.checkpoint.size</name>
    <value>3221225472</value>
    <description>The size of the current edit log (in bytes) that triggers
       a periodic checkpoint even if the fs.checkpoint.period hasn't expired.
  </description>
  </property>

  <!-- ipc properties: copied from kryptonite configuration -->
  <property>
    <name>ipc.client.idlethreshold</name>
    <value>8000</value>
    <description>Defines the threshold number of connections after which
               connections will be inspected for idleness.
  </description>
  </property>

  <property>
    <name>ipc.client.connection.maxidletime</name>
    <value>30000</value>
    <description>The maximum time after which a client will bring down the
               connection to the server.
  </description>
  </property>

  <property>
    <name>ipc.client.connect.max.retries</name>
    <value>50</value>
    <description>Defines the maximum number of retries for IPC connections.</description>
  </property>

  <!-- Web Interface Configuration -->
  <property>
    <name>webinterface.private.actions</name>
    <value>true</value>
    <description> If set to true, the web interfaces of JT and NN may contain
                actions, such as kill job, delete file, etc., that should
                not be exposed to public. Enable this option if the interfaces
                are only reachable by those who have the right authorization.
  </description>
  </property>


<!-- BEGIN security configuration -->
  <property>
    <name>hadoop.security.authentication</name>
    <value>kerberos</value>
    <!-- A value of "simple" would  disable security. -->
  </property>
  <property>
    <name>hadoop.security.authorization</name>
    <value>true</value>
  </property>

  <property>
    <name>hadoop.http.filter.initializers</name>
    <value>org.apache.hadoop.security.AuthenticationFilterInitializer</value>
  </property>

  <property>
    <name>hadoop.http.authentication.type</name>
    <value>org.apache.hadoop.security.authentication.server.PingFederateAuthenticationHandler</value>
  </property>

  <property>
    <name>hadoop.http.authentication.token.validity</name>
    <value>36000</value>
    <!-- in seconds -->
  </property>

  <property>
    <name>hadoop.http.authentication.signature.secret.file</name>
    <value>/etc/hadoop/http_auth_secret</value>
  </property>

  <property>
    <name>hadoop.http.authentication.cookie.domain</name>
    <value>ebay.com</value>
  </property>

  <property>
    <name>hadoop.http.authentication.pingFederate.config.file</name>
    <value>/etc/hadoop/pingfederate-agent-config.txt</value>
  </property>

  <property>
    <name>hadoop.http.authentication.pingFederate.url</name>
    <value>https://sso.corp.ebay.com/sp/startSSO.ping?PartnerIdpId=eBayHadoop</value>
  </property>
<property>
    <name>hadoop.http.authentication.pingFederate.anonymous.allowed</name>
    <value>true</value>
</property>

  <property>
    <name>hadoop.security.auth_to_local</name>
    <value>
        RULE:[1:$1]
        RULE:[2:$1]
        DEFAULT
    </value>
  </property>

  <property>
    <name>kerberos.multiplerealm.supported</name>
    <value>true</value>
  </property>
  <property>
    <name>kerberos.multiplerealm.realms</name>
    <value>CORP.EBAY.COM</value>
  </property>

<!-- HTTPS SUPPORT -->
<property>
  <name>hadoop.ssl.keystores.factory.class</name>
    <value>org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory</value>
      <description>
          The keystores factory to use for retrieving certificates.
            </description>
            </property>

<property>
  <name>hadoop.ssl.require.client.cert</name>
    <value>false</value>
      <description>Whether client certificates are required</description>
      </property>

<property>
  <name>hadoop.ssl.hostname.verifier</name>
    <value>ALLOW_ALL</value>
      <description>
          The hostname verifier to provide for HttpsURLConnections.
              Valid values are: DEFAULT, STRICT, STRICT_I6, DEFAULT_AND_LOCALHOST and
                  ALLOW_ALL
                    </description>
                    </property>

<property>
  <name>hadoop.ssl.server.conf</name>
    <value>ssl-server.xml</value>
      <description>
          Resource file from which ssl server keystore information will be extracted.
              This file is looked up in the classpath, typically it should be in Hadoop
                  conf/ directory.
                    </description>
                    </property>

<property>
  <name>hadoop.ssl.client.conf</name>
    <value>ssl-client.xml</value>
      <description>
          Resource file from which ssl client keystore information will be extracted
              This file is looked up in the classpath, typically it should be in Hadoop
                  conf/ directory.
                    </description>
                    </property>

<property>
  <name>hadoop.ssl.enabled</name>
    <value>true</value>
      <description>
          Whether to use SSL for the HTTP endpoints. If set to true, the
              NameNode, DataNode, ResourceManager, NodeManager, HistoryServer and
                  MapReduceAppMaster web UIs will be served over HTTPS instead HTTP.
                    </description>
                    </property>


<!-- END security configuration -->
</configuration>
