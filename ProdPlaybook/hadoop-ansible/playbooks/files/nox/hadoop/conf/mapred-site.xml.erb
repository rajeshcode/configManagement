<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>

  <property>
    <name>mapred.job.tracker</name>
    <value><%= cfg['mapred.job.tracker'] %></value>
  </property>

  <property>
    <name>mapred.local.dir</name>
    <value><%= mapred_local_dir.join(',') %></value>
    <final>true</final>
  </property>

  <property>
    <name>mapred.hosts.exclude</name>
    <value>/etc/hadoop/conf/mapred-exclude</value>
  </property>

  <property>
    <name>io.sort.mb</name>
    <value>512</value>
  </property>

  <property>
    <name>io.sort.factor</name>
    <value>100</value>
  </property>

  <property>
    <!-- 10,000 is 100 tasks per node on a 100-node cluster -->
    <name>mapred.jobtracker.maxtasks.per.job</name>
    <value>1000000</value>
    <final>true</final>
  </property>

  <property>
    <!-- Recover existing jobs on restart (may not be completely safe) -->
    <name>mapred.jobtracker.restart.recover</name>
    <value>true</value>
    <final>true</final>
  </property>

  <property>
    <name>mapred.child.java.opts</name>
    <value>-Xmx2g</value>
  </property>

  <property>
    <!-- set this to ~1.5x the heap size in mapred.child.java.opts -->
    <name>mapred.child.ulimit</name>
    <value>3145728</value> <!-- 3 GB in KB -->
  </property>

  <property>
    <!-- see http://hadoop.apache.org/common/docs/current/cluster_setup.html -->
    <name>mapred.job.tracker.handler.count</name>
    <value>64</value>
    <final>true</final>
  </property>

  <property>
	<name>mapred.map.tasks.speculative.execution</name>
    <value>true</value>
  </property>

  <property>
    <name>mapred.reduce.tasks.speculative.execution</name>
    <value>false</value>
  </property>

  <property>
    <name>mapred.reduce.parallel.copies</name>
    <!-- set this to somewhere between sqrt(nodes) and nodes/2.
	 for <20 nodes, set == |nodes| -->
    <value>36</value>
  </property>

  <property>
	<name>mapred.reduce.tasks</name>
    <!-- set to numnodes * mapred.tasktracker.reduce.tasks.maximum -->
    <value><%= cfg['mapred.reduce.tasks'] %></value>
  </property>

  <property>
    <name>mapred.tasktracker.map.tasks.maximum</name>
    <!-- see other kb entry about this one. -->
    <value>4</value>
    <final>true</final>
  </property>

  <property>
    <name>mapred.tasktracker.reduce.tasks.maximum</name>
    <!-- see other kb entry about this one. -->
    <value>2</value>
    <final>true</final>
  </property>

  <property>
    <name>tasktracker.http.threads</name>
    <value>80</value>
    <final>true</final>
  </property>

  <property>
    <name>mapred.output.compression.type</name>
    <value>BLOCK</value>
    <description>If the job outputs are to compressed as
      SequenceFiles, how should they be compressed? Should be one of
      NONE, RECORD or BLOCK. Cloudera's Distribution for Hadoop
      switches this default to BLOCK for better
      performance.</description>
  </property>

  <property>
    <name>mapred.output.compress</name>
    <value>true</value>
    <description>Do we want compressed output?  Set it to false initially</description>
  </property>

  <!-- LZO: see http://www.facebook.com/notes/cloudera/hadoop-at-twitter-part-1-splittable-lzo-compression/178581952002 -->
  <property>
    <name>mapred.map.output.compression.codec</name>
         <value>com.hadoop.compression.lzo.LzoCodec</value>
         <!-- <value>org.apache.hadoop.io.compress.GzipCodec</value> -->
    <description>What compression algorithm to use?</description>
  </property>

  <property>
    <name>mapred.output.compression.codec</name>
    <value>org.apache.hadoop.io.compress.GzipCodec</value>
	<description>If the job outputs are compressed, how should they be compressed?</description>
  </property>

  <property>
    <name>mapred.jobtracker.retirejob.interval</name>
    <value>28800000</value> <!-- 8 hours in milliseconds -->
    <description>An undocumented feature that controls the number of hours job
      history objects are kept in the memory of the jobtracker.  This is in milliseconds.
    </description>
    <final>true</final>
  </property>

  <property>
    <name>mapred.job.tracker.persist.jobstatus.active</name>
    <value>true</value>
    <description>Indicates if persistency of job status information is active or not. </description>
  </property>

  <property>
    <name>mapred.job.tracker.persist.jobstatus.hours</name>
    <value>720</value>
    <description>The number of hours job status information is persisted in DFS.
      The job status information will be available after it drops of the memory
      queue and between jobtracker restarts. With a zero value the job status
      information is not persisted at all in DFS.  By default these data will
      be stored in /jobtracker/jobsInfo.  See mapred.job.tracker.persist.jobstatus.dir
    </description>
  </property>

  <property>
    <name>mapred.userlog.retain.hours</name>
    <value>48</value>
    <description>The maximum time, in hours, for which the user-logs are to be retained.</description>
  </property>

  <property>
    <name>mapred.jobtracker.completeuserjobs.maximum</name>
    <value>3</value>
    <description>The maximum number of complete jobs per user to keep around 
    before delegating them to the job history.</description>
  </property>

  <property>
    <name>mapred.reduce.slowstart.completed.maps</name>
    <value>0.80</value>
    <description>The proportion of map tasks in a job that need to be complete before any reduce tasks are scheduled</description>
  <final>true</final>
  </property>
  
  <property>
    <name>mapreduce.jobtracker.split.metainfo.maxsize</name>
    <value>1000000000</value>
    <description>property overridden to help service Split metadata size exceeded exception</description>
    <final>true</final>
  </property>

<% if cfg['use.capacity_scheduler'] %>
  <!-- The Capacity Scheduler relies on groups -->

  <property>
     <name>mapreduce.jobtracker.taskscheduler</name>
     <value>org.apache.hadoop.mapred.CapacityTaskScheduler</value>
  </property>

  <!-- queue names --> 
  <property>
    <name>mapred.queue.names</name>
    <value>hdc_uc4_platform,hdmi-technology,hdmi-research,hdmi-set,hdmi-mm,hdmi-hadoopeng,hdmi-prod,hdmi-others</value>
  </property>

  <!-- enable ACLs for queues --> 
  <property> 
    <name>mapred.acls.enabled</name> 
    <value>true</value> 
  </property>

<% end %>

  <property>
    <name>mapreduce.job.counters.limit</name>
    <value>10000</value>
  </property>

  <!--
  <property>
    <name>mapreduce.jobtracker.jobhistory.location</name>
    <value>/jthistory</value>
    <description> If job tracker is static the history files are stored 
    in this single well known place. If No value is set here, by default,
    it is in the local file system at ${hadoop.log.dir}/history.
    </description>
  </property>
  -->

  <property>
    <name>mapred.job.tracker.history.completed.location</name>
    <value>/jthistory</value>
    <description> The completed job history files are stored at this single well
    known location. If nothing is specified, the files are stored at
    ${mapreduce.jobtracker.jobhistory.location}/done.
    </description>
  </property>

  <!--
    FAIR SCHEDULER
  -->

  <property>
    <name>mapred.fairscheduler.preemption</name>
    <value>true</value>
  </property>

  <property>
    <name>mapred.fairscheduler.poolnameproperty</name>
    <value>mapreduce.job.queuename</value>
  </property>

  <property>
    <name>mapred.fairscheduler.allow.undeclared.pools</name>
    <value>false</value>
  </property>

</configuration>
