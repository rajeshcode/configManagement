<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  
  <property>
    <!-- URI of NN. Fully qualified. No IP.-->
    <name>fs.default.name</name>
    <value>hdfs://nox-srw-nn.vip.ebay.com:8020</value>
  </property>

  <property>
    <!-- Amount of data buffered in r/w operations. Multiple of hw pages in bytes. -->
    <name>io.file.buffer.size</name>
    <value>131072</value> <!-- 128 KB -->
  </property>
  
  <property>
    <description>If users connect through a SOCKS proxy, we don't
      want their SocketFactory settings interfering with the socket
      factory associated with the actual daemons.</description>
    <name>hadoop.rpc.socket.factory.class.default</name>
    <value>org.apache.hadoop.net.StandardSocketFactory</value>
    <final>true</final>
  </property>
  
  <property>
    <description>Topology script</description>
    <name>topology.script.file.name</name>
    <value>/usr/local/bin/net-topology</value>
    <final>true</final>
  </property>

  <property>
    <name>hadoop.tmp.dir</name>
	<value>/tmp/hadoop/hadoop-${user.name}</value>
    <description>A base for other temporary directories.</description>
  </property>


  <property>
    <name>hadoop.rpc.socket.factory.class.ClientProtocol</name>
    <value></value>
    <final>true</final>
  </property>
  
  <property>
    <name>hadoop.rpc.socket.factory.class.JobSubmissionProtocol</name>
    <value></value>
    <final>true</final>
  </property>

  <property>
    <name>io.compression.codecs</name>
    <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.BZip2Codec</value>
    <!-- <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec</value> -->
    <final>true</final>
  </property>

  <!-- LZO: see http://www.facebook.com/notes/cloudera/hadoop-at-twitter-part-1-splittable-lzo-compression/178581952002 -->
  <property>
    <name>io.compression.codec.lzo.class</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
  </property>

  <property>
    <name>fs.trash.interval</name>
    <value>0</value>
    <description>Number of minutes between trash checkpoints.
    If zero, the trash feature is disabled.
    </description>
  </property>

<!-- mobius-proxyagent impersonation configurations -->
<property>
    <name>hadoop.proxyuser.mobius-proxyagent.groups</name>
    <value>hdmi-mm,hdmi-set,hdmi-research,hdmi-technology,hdmi-hadoopeng,hdmi-cs,hdmi-milo,hdmi-appdev,hdmi-siteanalytics,hdmi-prod,hdmi-others,hdmi-sdc,hdmi-mobile</value>
    <description>Allow user mobius-proxyagent to impersonate any members of the groups </description>
</property>
 
<property>
    <name>hadoop.proxyuser.mobius-proxyagent.hosts</name>
    <value>10.102.5.20,10.103.102.36,10.103.102.203</value>
   <description>The mobius-proxyagent can connect from hosts to impersonate a user</description>
</property>
<property>
    <name>hadoop.proxyuser.test_batch_job.groups</name>
    <value>hdmi-mm,hdmi-set,hdmi-research,hdmi-technology,hdmi-hadoopeng,hdmi-cs,hdmi-milo,hdmi-appdev,hdmi-siteanalytics,hdmi-prod,hdmi-others,hdmi-sdc,hdmi-mobile</value>
    <description>Allow user mobius-proxyagent to impersonate any members of the groups </description>
</property>

<property>
    <name>hadoop.proxyuser.test_batch_job.hosts</name>
    <value>10.110.41.12</value>
   <description>The mobius-proxyagent can connect from hosts to impersonate a user</description>
</property>

<!-- security config -->


 <property>
      <name>hadoop.security.authentication</name>
    <!--  <value>simple</value>-->
      <value>kerberos</value> 
                            <!-- A value of "simple" would  disable security. -->
</property>
<property>
    <name>hadoop.security.authorization</name>
            <value>true</value>
                    </property>


<!-- security config ends here -->
 <!--  WEB UI security -->

<property>
    <name>hadoop.http.filter.initializers</name>
        <value>org.apache.hadoop.security.AuthenticationFilterInitializer</value>
        </property>

<property>
    <name>hadoop.http.authentication.type</name>
        <value>org.apache.hadoop.security.authentication.server.PingFederateAuthenticationHandler</value>
<!--
    <value>org.apache.hadoop.security.authentication.server.AnonymousAuthenticationHandler</value>
-->
        </property>

<property>
    <name>hadoop.http.authentication.token.validity</name>
        <value>36000</value>
           <!-- in seconds -->
</property>

<property>
    <name>hadoop.http.authentication.signature.secret.file</name>
            <value>/etc/hadoop/http_auth_secret</value>
                    </property>

<property>
    <name>hadoop.http.authentication.cookie.domain</name>
        <value>ebay.com</value>
        </property>
<property>
    <name>hadoop.http.authentication.pingFederate.config.file</name>
        <value>/apache/hadoop/conf/pingfederate-agent-config.txt</value>
</property>
<property>
    <name>hadoop.http.authentication.pingFederate.url</name>
        <value>https://ssodev.corp.ebay.com/sp/startSSO.ping?PartnerIdpId=APDDEV</value>
        </property>  
        <!-- WEB UI Security config ends here -->
<property>
    <name>hadoop.security.authentication.userrealm</name>
        <value>CORP.EBAY.COM</value>
        </property>

<property>
    <name>hadoop.security.auth_to_local</name>
    <value>  
        RULE:[1:$1]
        RULE:[2:$1]
        DEFAULT
    </value>
</property>

<!-- HTTPS SUPPORT -->

<property>
  <name>hadoop.ssl.keystores.factory.class</name>
    <value>org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory</value>
      <description>
          The keystores factory to use for retrieving certificates.
            </description>
            </property>

<property>
  <name>hadoop.ssl.require.client.cert</name>
    <value>false</value>
      <description>Whether client certificates are required</description>
      </property>

<property>
  <name>hadoop.ssl.hostname.verifier</name>
    <value>ALLOW_ALL</value>
      <description>
          The hostname verifier to provide for HttpsURLConnections.
              Valid values are: DEFAULT, STRICT, STRICT_I6, DEFAULT_AND_LOCALHOST and
                  ALLOW_ALL
                    </description>
                    </property>

<property>
  <name>hadoop.ssl.server.conf</name>
    <value>ssl-server.xml</value>
      <description>
          Resource file from which ssl server keystore information will be extracted.
              This file is looked up in the classpath, typically it should be in Hadoop
                  conf/ directory.
                    </description>
                    </property>

<property>
  <name>hadoop.ssl.client.conf</name>
    <value>ssl-client.xml</value>
      <description>
          Resource file from which ssl client keystore information will be extracted
              This file is looked up in the classpath, typically it should be in Hadoop
                  conf/ directory.
                    </description>
                    </property>

<property>
  <name>hadoop.ssl.enabled</name>
    <value>true</value>
      <description>
          Whether to use SSL for the HTTP endpoints. If set to true, the
              NameNode, DataNode, ResourceManager, NodeManager, HistoryServer and
                  MapReduceAppMaster web UIs will be served over HTTPS instead HTTP.
                    </description>
                    </property>

</configuration>

