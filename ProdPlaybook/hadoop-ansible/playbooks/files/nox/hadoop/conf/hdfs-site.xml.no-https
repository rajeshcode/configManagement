<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<property>
<name>dfs.data.dir</name>
<value>/hadoop03/data,/hadoop10/data,/hadoop11/data,/hadoop07/data,/hadoop04/data,/hadoop01/data,/hadoop05/data,/hadoop02/data,/hadoop09/data,/hadoop06/data,/hadoop08/data,/hadoop12/data</value>
<final>true</final>
</property>

<property>
    <name>dfs.hosts</name>
    <!-- The files containing hosts allowed to connect to namenode -->
    <value>/etc/hadoop/conf/hosts</value>
  </property>

  <property>
    <name>dfs.hosts.exclude</name>
    <!-- The files containing hosts allowed to connect to namenode -->
    <value>/etc/hadoop/conf/hdfs-exclude</value>
  </property>

  <property>
    <name>dfs.namenode.handler.count</name>
    <value>128</value>
    <final>true</final>
  </property>

  <property>
    <name>dfs.datanode.handler.count</name>
    <value>12</value>
    <final>true</final>
  </property>

  <property>
    <!-- Number of simultaneously open connections (default 256) -->
    <!-- <name>dfs.datanode.max.transfer.threads</name> -->
    <name>dfs.datanode.max.xcievers</name>
    <value>16000</value>
    <final>true</final>
  </property>

  <!-- name node -->
  <property>
    <!-- Where on the local fs the NN stores its data -->
    <name>dfs.name.dir</name>
    <value>/hadoop/nn1/1,/hadoop/nn1/2</value>
    <final>true</final>
  </property>

  <!-- secondary name node -->
  <property>
    <!-- Where on the local fs the 2NN stores its data -->
    <name>dfs.namenode.checkpoint.dir</name>
    <value>/hadoop/nn2/1,/hadoop/nn2/2</value>
    <final>true</final>
  </property>

  <property>
    <name>dfs.block.size</name>
    <!-- 128mb (default 64m or 67108864) -->
    <value>134217728</value>
  </property>

  <property>
    <!-- Amount of space which HDFS will refuse to use in bytes -->
    <name>dfs.datanode.du.reserved</name>
    <value>300000000</value> <!-- 200 GB-->
    <final>true</final>
  </property>

  <property>
    <name>dfs.permissions</name>
    <value>true</value>
    <final>true</final>
  </property>

  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
<!-- RMERCHIA AISOPS159160 2012-09-25 -->
<property>
  <name>dfs.heartbeat.interval</name>
  <value>6</value>
  <final>true</final>
  <description>how frequently dn send a heartbeat.</description>
</property>

<!-- RMERCHIA AISOPS159160 2012-09-25  change to 6 hours on 2012-10-02 -->
<property>
  <name>dfs.blockreport.intervalMsec</name>
  <value>21600000</value>
  <final>true</final>
  <description>how frequently dn send a blockreport.</description>
</property>

  <property>
    <name>dfs.safemode.threshold.pct</name>
    <value>0.80</value>
    <!-- Allows 10 blocks unreported out of 10,000,000 -->
    <description>
    Specifies the percentage of blocks that should satisfy 
    the minimal replication requirement defined by dfs.replication.min.
    Values less than or equal to 0 mean not to start in safe mode.
    Values greater than 1 will make safe mode permanent.
    </description>
  </property>

  <property>
    <name>dfs.safemode.extension</name>
    <value>120000</value>
    <!-- 2 minutes -->
    <description> Determines extension of safe mode in milliseconds after the threshold level is reached. </description>
  </property>

  <property>
    <!-- 100Mbit/s -->
    <name>dfs.balance.bandwidthPerSec</name>
    <value>104857600</value>
  </property>

<property>
   <name>dfs.datanode.failed.volumes.tolerated</name>
   <value>3</value>
</property>

<property>
    <name>dfs.block.local-path-access.user</name>
    <value>hadoop</value>
    <description>the user who is allowed to perform short
    circuit reads.
    </description>
    <final>true</final>
</property>
<!--
<property>
  <name>fs.checkpoint.period</name>
  <value>60</value>
</property>
-->
  <property>
     <name>dfs.http.address</name>
     <value>nox-srw-nn.vip.ebay.com:50070</value>
     <description>
     The address and the base port where the dfs namenode web ui will listen on.
     If the port is 0 then the server will start on a free port.
     </description>
   </property>
 
 <property>
     <name>dfs.secondary.http.address</name>
     <value>nox-srw-sn.vip.ebay.com:50090</value>
     <description>
     The secondary namenode http server address and port.
     If the port is 0 then the server will start on a free port.
     </description>
   </property>
<property>
  <name>dfs.web.authentication.kerberos.keytab</name>
  <value>/etc/hadoop/hadoop.keytab</value>
</property>

<property>
<name>dfs.block.access.token.enable</name>
<value>true</value>
</property>

<!-- NameNode security config -->

<property>
  <name>dfs.namenode.kerberos.internal.spnego.principal</name>
  <value>HTTP/_HOST@APDTEST.EBAY.COM</value>
</property>
<property>
  <name>dfs.namenode.keytab.file</name>
  <value>/etc/hadoop/hadoop.keytab</value>
</property>
<property>
  <name>dfs.namenode.kerberos.principal</name>
  <value>hadoop/_HOST@APDTEST.EBAY.COM</value>
<!-- _HOST will be replaced by the the domain name present in fs.default.name. It is better to use the actual host name  -->
</property>
<!-- Secondary NameNode security config -->
<property>
  <name>dfs.secondary.namenode.kerberos.internal.spnego.principal</name>
  <value>HTTP/_HOST@APDTEST.EBAY.COM</value>
</property>
<property>
  <name>dfs.secondary.namenode.keytab.file</name>
  <value>/etc/hadoop/hadoop.keytab</value>
</property>
<property>
  <name>dfs.secondary.namenode.kerberos.principal</name>
  <value>hadoop/_HOST@APDTEST.EBAY.COM</value>
</property>
<!-- DataNode security config -->
<property>
  <name>dfs.datanode.data.dir.perm</name>
  <value>700</value>
  </property>
<property>
  <name>dfs.datanode.address</name>
  <value>0.0.0.0:1004</value>
</property>
<property>
  <name>dfs.datanode.http.address</name>
  <value>0.0.0.0:1006</value>
</property>
<property>
  <name>dfs.datanode.keytab.file</name>
  <value>/etc/hadoop/hadoop.keytab</value>
</property>
<property>
  <name>dfs.datanode.kerberos.principal</name>
  <value>hadoop/_HOST@APDTEST.EBAY.COM</value>
  <!-- _HOST will be replaced by the frst domain name mapped to the ip -->
</property>

<property>
  <name>dfs.support.append</name>
  <value>true</value>
</property>

<!-- General HDFS security config -->
<property>
<name>dfs.block.access.token.enable</name>
<value>true</value>
</property>

</configuration>
